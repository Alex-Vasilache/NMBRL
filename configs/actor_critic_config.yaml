# Training
num_epochs: 100000
batch_size: 16
batch_length: 100
learning_rate: 3e-5
eps: 1e-5
grad_clip: 100.0 # try 1.0
max_steps_per_episode: 1000
device: 'global' # Uses global.device setting
reward_EMA: True
seed: 42
deterministic_run: False


# Behavior
gamma: 0.997            # Discount factor for the reward
discount_lambda: 0.95   # return mixing factor
imag_horizon: 15        # Horizon for imagined trajectories

# Simulation 
visualize: False
dt_simulation: 0.02

# Model
act: 'SiLU'             # Activation function
norm: True              # Layer Normalization
units: 128               # Number of neurons in each layer # todo 32-64

# Logging
save_dir: "saved_models"  # Directory to save trained models
log_dir: "runs"  # Directory for TensorBoard logs
save_frequency: 1000  # Save models every N episodes (None to disable)
eval_frequency: 100  # Evaluate every N episodes (None to disable)
print_frequency: 10  # Print every N episodes (None to disable)

# Evaluation (runs at same frequency as saves)
eval_episodes: 5  # Number of episodes for intermediate evaluation
eval_visualize: false  # Whether to visualize during intermediate evaluation
final_eval_episodes: 100  # Number of episodes for final evaluation

# Actor
actor:
  layers: 2 # todo 2
  dist: 'normal' # maybe TanhBijector or tanh_normal
  entropy: 3e-4
  unimix_ratio: 0.01
  std: 'learned'
  min_std: 0.1
  max_std: 1.0
  temp: 0.1
  outscale: 1.0  # try 0.01
  imag_grad: 'reinforce'

# Critic
critic:
  layers: 2 # todo 2
  dist: 'symlog_disc'
  outscale: 0.0
  slow_target: True
  slow_target_update: 1
  slow_target_fraction: 0.02