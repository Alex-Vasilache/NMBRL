# Global configuration for the entire run
global:
  seed: 41
  run_folder_prefix: "runs"
  env_type: "dmc" # "dmc" or "physical"
  device: "auto" # "auto", "cuda", "cpu" - auto will use cuda if available, otherwise cpu
  
# TensorBoard logging configuration
tensorboard:
  log_dir: "tb_logs"
  log_frequency: 10  # Log every N steps/episodes
  flush_seconds: 30  # Flush logs every N seconds

# Configuration for the main run_full_system.py script
run_system:
  head_start_seconds: 5
  create_new_consoles: false # If false, output will be piped to the main console

# Configuration for learning/dynamic_data_generator.py
data_generator:
  max_episode_steps: 1000
  actor_check_interval_seconds: 1
  buffer_write_interval_seconds: 1
  dt_simulation: 0.02 # todo integrate into the sim
  render_enabled: true  # Set to false for headless environments, true for local development

# Configuration for learning/dynamic_train_world_model.py
world_model_trainer:
  max_valid_init_buffer_size: 1000000  # Adjust as needed
  # Training trigger settings
  new_data_threshold: 1000
  watcher_interval_seconds: 1
  # Model architecture
  hidden_dim: 32
  # Training parameters
  buffer_policy: "oldest_n" # "latest" or "random", "oldest_n"
  batch_size: "all" # "all" or an integer value
  epochs_per_cycle: 10000
  learning_rate: 0.0004
  validation_split: 0.2
  # Learning rate scheduler settings
  lr_patience: 5
  lr_factor: 0.3
  use_scalers: True
  use_output_state_scaler: False # Agent networks will received normalizedstates in range [-3, 3] if False
  use_output_reward_scaler: True
  # RNN-specific settings
  model_type: "rnn" # "mlp" or "rnn"
  num_layers: 2
  dropout: 0.3
  context_length: 5   # Number of context frames given to the model
  prediction_length: 45 # Number of future frames to predict (context_length + prediction_length = imag_horizon)

# Configuration for learning/dynamic_train_agent_sb3.py
agent_trainer:
  agent_type: "DREAMER" # "PPO" or "SAC" or "DREAMER" or "EVO"
  # Environment settings
  n_envs: 16
  max_episode_steps: 16
  # Training run settings
  total_timesteps: 10000000
  # Callback settings
  checkpoint_freq: 1000
  verbose: 0

  # Settings for the world model wrapper used by the SAC agent
  world_model_wrapper:
    model_check_interval_s: 1
    obs_clip_range: 100.0
    reward_clip_range: [0.0, 1.0]

evo_agent_trainer:
  # Spatial-SNNs Configuration File
  # Configuration for evolving spatially embedded recurrent spiking neural networks for control tasks

  # =================================================================
  # GENERAL CONFIGURATION
  # =================================================================
  general:
    checkpoint_path: null  # Path to directory containing saved model checkpoint for loading pretrained weights
    test: false  # Enable test mode: skip training and only run inference/visualization with existing checkpoint
    random_seed: 0  # Random seed for reproducible experiments (affects network initialization and environment)
    device: "cpu"  # Computing device: GPU ID (e.g., '0', '1') or 'cpu' for CPU-only execution

  # =================================================================
  # ENVIRONMENT CONFIGURATION
  # =================================================================
  environment:
    game_name: "CartPole-v1"  # Gymnasium environment name (e.g., 'CartPole-v1', 'Hopper-v4', 'Ant-v4')
    visualization: true  # Enable real-time visualization of agent performance during training/testing
    max_env_steps: 15  # Maximum number of environment steps per episode before automatic reset
    discretize_intervals: 0  # For continuous action spaces: number of discrete intervals per action dimension. Set to 0 or 1 to keep actions continuous

  # =================================================================
  # NETWORK ARCHITECTURE
  # =================================================================
  network:
    net_size: [4, 4, 1]  # 3D grid dimensions for hidden layer neurons: [width, height, depth]. Total hidden neurons = width × height × depth
    spike_steps: 4  # Number of SNN simulation timesteps per environment step (higher = more temporal dynamics)
    max_vthr: 1000  # Maximum membrane potential threshold for LIF neurons (controls spiking sensitivity)
    spatial: true  # Enable spatial embedding: connection probabilities decay with distance in 3D space
    prune_unconnected: false  # Remove isolated neurons and disconnected subgraphs to improve efficiency

  # =================================================================
  # EVOLUTIONARY ALGORITHM PARAMETERS
  # =================================================================
  evolution:
    num_iterations: 1000  # Total number of evolutionary generations to run
    num_gene_samples: 16  # Population size: number of individual networks evaluated per generation
    evolution_method: "classic"  # Evolutionary strategy: 'classic' (standard GA) or 'map_elites' (quality-diversity)

  # =================================================================
  # MAP-ELITES SPECIFIC PARAMETERS
  # =================================================================
  map_elites:
    sigma_bins: 10  # MAP-Elites: Number of bins for first behavioral descriptor (connection strength diversity)
    sparsity_bins: 10  # MAP-Elites: Number of bins for second behavioral descriptor (network sparsity)

  # =================================================================
  # TRAINING AND EVALUATION
  # =================================================================
  training:
    batch_size_gene: 16  # Number of network configurations to evaluate in parallel on GPU (memory permitting)
    num_data_samples: 1000  # Total number of evaluation episodes per network (higher = more reliable fitness estimate)
    batch_size_data: 1000  # Number of parallel environments to run simultaneously per network evaluation
    curiculum_learning: false  # Gradually increase episode length by 1.1x every generation for progressive difficulty. Capped at 1000 steps. 

# Configuration for learning/dynamic_train_agent_dreamer.py
dreamer_agent_trainer:
  # Training
  num_epochs: 1000000
  batch_size: 16
  learning_rate: 4e-4
  eps: 1e-5
  grad_clip: 1.0 # try 1.0
  reward_EMA: True
  deterministic_run: False
  device: "global" # Uses global.device setting
  loss_aggregation: "mean" # "mean" or "sum"

  # Behavior
  gamma: 0.997            # Discount factor for the reward
  discount_lambda: 0.95   # return mixing factor
  imag_horizon: 15        # Horizon for imagined trajectories

    # Model
  act: 'SiLU'             # Activation function
  norm: True              # Layer Normalization
  units: 32               # Number of neurons in each layer # todo 32-64

  # Logging
  save_frequency: 1000  # Save models every N episodes (None to disable)
  eval_frequency: 200  # Evaluate every N episodes (None to disable)
  print_frequency: 100  # Print every N episodes (None to disable)

  # Evaluation (runs at same frequency as saves)
  eval_episodes: 5  # Number of episodes for intermediate evaluation
  final_eval_episodes: 100  # Number of episodes for final evaluation

  # Actor
  actor:
    layers: 2 # todo 2
    dist: 'normal' # maybe TanhBijector or tanh_normal
    entropy: 3e-4
    unimix_ratio: 0.01
    std: 'learned'
    min_std: 0.1
    max_std: 1.0
    temp: 0.1
    outscale: 1.0  # try 0.01
    imag_grad: 'reinforce'

  # Critic
  critic:
    layers: 2 # todo 2
    dist: 'symlog_disc'
    outscale: 0.0
    slow_target: True
    slow_target_update: 1
    slow_target_fraction: 0.02